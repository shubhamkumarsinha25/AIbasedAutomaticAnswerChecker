from nltk import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import numpy as np

from nltk.stem import WordNetLemmatizer

#%%
#gensim library
def preprocess(docx):    
    lemmatizer=WordNetLemmatizer()
    docx=docx.lower()
    docx=re.sub('[^a-zA-Z .]',' ',docx)
    all_words=word_tokenize(docx)
    root_words=[lemmatizer.lemmatize(j) for j in all_words]
    string=' '.join(root_words)
    
    return(string)
    
#%%
#n-gram(Bigram) model
def ngram(corpus):
    new_vocab=word_tokenize(corpus)
    fd=FreqDist(new_vocab)
    words = [w for w in fd.keys()]
    most=[]
    for wr in words:
        most.append([wr,fd[wr]])
    most=sorted(most,key=lambda i:i[1],reverse=True)
    most=[a for a in most if a[0] not in set(stopwords.words('english')) ]
    most=most[:10]
  
    x=[]
    for b,c in most:
        x.append(b)
    
    new=[]
    for i in x:
        new.append([i,[n for n,val in enumerate(new_vocab) if val==i]])
    y=[]
    for j,k in new:
        y.append(k)

#bi-gram model:    
    master_prev=[]
    for z in y:
        for p in z:
            master_prev.append([(new_vocab[p-1]),(new_vocab[p])])        
    try:
       master_next=[]
       for z in y:
           for p in z:
               master_next.append([(new_vocab[p]),(new_vocab[p+1])])        
    except IndexError as error:
        pass
    master=master_next + master_prev
    dictionary=[]
    for ke in master:
        if ke not in dictionary:
            dictionary.append(ke)

    bigram=[]
    c=0
    for word in dictionary:
        for sec in master:
            if word==sec:
                c=c+1
        bigram.append([word,c])   
        c=0    
    bigram=sorted(bigram,key=lambda s:s[1],reverse=True) 
    bigram=[w for w in bigram if w[1]>1]
    
#tri-gram model    
    tri_prev=[]
    for z in y:
        for p in z:
            tri_prev.append([(new_vocab[p-2]),(new_vocab[p-1]),(new_vocab[p])])        
   
    try:
        tri_next=[]
        for z in y:
            for p in z:
                tri_next.append([(new_vocab[p]),(new_vocab[p+1]),(new_vocab[p+2])])       
    except IndexError as error:
        pass           

    tri=tri_next + tri_prev
    dictionary_=[]
    for lk in tri:
        if lk not in dictionary_:
            dictionary_.append(lk)

    trigram=[]
    ct=0
    for word in dictionary_:
        for sec in tri:
            if word==sec:
                ct=ct+1
        trigram.append([word,ct])   
        ct=0
                
    trigram=sorted(trigram,key=lambda s:s[1],reverse=True)    
    trigram=trigram[:10]
    
    return(trigram,bigram)        


#%%
#gensim sumarizer
def summary(stry):
    from gensim.summarization.summarizer import summarize
    summary=summarize(stry,word_count=100)
    return summary


#%%
# cosine similarity calculation
def cossim(doc1,doc2):
    from sklearn.metrics.pairwise import cosine_similarity as cs
    from sklearn.feature_extraction.text import CountVectorizer as cv

    x=[doc1,doc2]
    vectorizer=cv().fit_transform(x)
    vectors=vectorizer.toarray()    
    
    a=vectors[0].reshape(1,-1)
    b=vectors[1].reshape(1,-1)    
    
    similarity_score=cs(a,b)        
    
    return similarity_score

    

#%%
def eval_marks(inst_1,inst_2):

    summary_1=summary(inst_1)
    trigram_1,bigram_1=ngram(inst_1)
    
    summary_2=summary(inst_2)
    trigram_2,bigram_2=ngram(inst_2)
    
    
    aux_bigram_1=[]
    for i in bigram_1:
        aux_bigram_1.append(i[0])
    aux_bigram_2=[]
    for j in bigram_2:
        aux_bigram_2.append(j[0])
    bigram_match=np.in1d(aux_bigram_1,aux_bigram_2)
    num_match_bigram=(bigram_match==True).sum()

    clean_summary_1=preprocess(summary_1)
    clean_summary_2=preprocess(summary_2)
    
    score=cossim(clean_summary_1,clean_summary_2)
    score=score[0][0]
    
    #lst=[num_match_bigram,num_match_doc,num_match_summ,score]

    marks_sim=(score/0.61)*2.2 
    if marks_sim>3.5:
        marks_sim=3.5
    marks_bi=(num_match_bigram/16)*1.2
    if marks_bi>1.5:
        marks_bi=1.5
    marks=marks_sim+marks_bi
    marks=round(marks)
    print(marks)
    
    return marks    


# Task A
inst_5='Inheritance is a method of forming new classes using predefined classes. The new classes are called derived classes and they inherit the behaviours and attributes of the base classes. It was intended to allow existing code to be used again with minimal or no alteration. It also offers support for representation by categorization in computer languages; this is a powerful mechanism of information processing, vital to human learning by means of generalization and cognitive economy. Inheritance is occasionally referred to as generalization due to the fact that is-a relationships represent a hierarchy between classes of objects. Inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code. Due to this, inheritance has another view called polymorphism, where many sections of code are being controlled by some shared control code. Inheritance is normally achieved by overriding one or more methods exposed by ancestor, or by creating new methods on top of those exposed by an ancestor. Inheritance has a variety of uses. Each different use focuses on different properties, for example the external behaviour of objects, internal structure of an object, inheritance hierarchy structure, or software engineering properties of inheritance. Occasionally it is advantageous to differentiate between these uses, as it is not necessarily noticeable from context. '
inst_7='In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula. The new classes, known as derived classes, take over (or inherit) attribute and behaviour of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification. Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.'
inst_10='The idea of inheritance in OOP refers to the formation of new classes with the already existing classes. The concept of inheritance was basically formulated for Simula in 1967. As a result, the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived. These original classes are either called base classes or sometimes referred to as ancestor classes. The idea of inheritance is to reuse the existing code with little or no modification at all. The basic support provided by inheritance is that it represents by categorization in computer languages. The power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization. Where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created. On the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities. There are examples where we can have modules with similar interfaces. The advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program.'
orig='In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula. The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification. Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor. Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.'




# # Task A
# inst_5='Inheritance is a method of forming new classes using predefined classes. The new classes are called derived classes and they inherit the behaviours and attributes of the base classes. It was intended to allow existing code to be used again with minimal or no alteration. It also offers support for representation by categorization in computer languages; this is a powerful mechanism of information processing, vital to human learning by means of generalization and cognitive economy. Inheritance is occasionally referred to as generalization due to the fact that is-a relationships represent a hierarchy between classes of objects. Inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code. Due to this, inheritance has another view called polymorphism, where many sections of code are being controlled by some shared control code. Inheritance is normally achieved by overriding one or more methods exposed by ancestor, or by creating new methods on top of those exposed by an ancestor. Inheritance has a variety of uses. Each different use focuses on different properties, for example the external behaviour of objects, internal structure of an object, inheritance hierarchy structure, or software engineering properties of inheritance. Occasionally it is advantageous to differentiate between these uses, as it is not necessarily noticeable from context. '
# inst_7='In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula. The new classes, known as derived classes, take over (or inherit) attribute and behaviour of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification. Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.'
# inst_10='The idea of inheritance in OOP refers to the formation of new classes with the already existing classes. The concept of inheritance was basically formulated for Simula in 1967. As a result, the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived. These original classes are either called base classes or sometimes referred to as ancestor classes. The idea of inheritance is to reuse the existing code with little or no modification at all. The basic support provided by inheritance is that it represents by categorization in computer languages. The power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization. Where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created. On the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities. There are examples where we can have modules with similar interfaces. The advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program.'
# orig='In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula. The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification. Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor. Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.'

# #%% 
# # Task B
# b__1='PageRank algorithm is patented by Stanford University. It is a link analysis algorithm employed by the Google Internet search engine that assigns a value used to measure the importance to each element of a hyperlinked set of documents, such as the WWW, with the purpose of ” measuring" its relative significance within the set. Google owns exclusive license rights on the patent from Stanford University. The University received 1.8 million shares in Google in return for use of the patent. '
# b__2='PageRank is a link analysis algorithm that is used by search engine such as Google Internet that assigns a numerical weighting to every element of a hyperlinked set of documents, like the World Wide Web, with the hope of "measuring" the relative importance held in the set. The algorithm may be applied to any numbr of entities with reciprocal quotations and references. The weight taking a numerical value which assigns to any given element E is also known as the PageRank of E and is denoted by PR(E). A trademark of Google has the name "PageRank"  and this process has been patented (U.S. Patent 6,285,999 ). Nevertheless, the patent is assigned to the University of Stanford and not to Google. Google has exclusive license rights on the patent from the University of Stanford and the university received 1.8 million shares in Google in exchange for use of the patent; the in the year 2005, shares were sold for $336 million.'
# b__3='PageRankalgorithm is also known as link analysis algorithm.  It has been used by google. The algorithm may be applied to any collection of entities with reciprocal quotations and hyperlinked set of documents, such as the World Wide Web, with the purpose of "measuring references. The name "PageRank" is a trademark of Google, and the PageRank process has been patented (U.S. Patent 6,285,999 ). The numerical weight that it assigns to any given element E is also called the PageRank of E and denoted by PR(E). The name "PageRank" is a trademark of Google, and the PageRank process has been patented (U.S. Patent 6,285,999 ). However, the patent is assigned to Stanford University and not to Google. Google has exclusive license rights on the patent from Stanford University. In other words, a PageRank results from a "ballot" among all the other pages on the World Wide Web about how important a page is. A hyperlink to a page counts as a vote of support. The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it ("incoming links"). Numerous academic papers concerning PageRank have been published since Page and Brins original paper.[4] In practice, the PageRank concept has proven to be vulnerable to manipulation, and extensive research has been devoted to identifying falsely inflated PageRank and ways to ignore links from documents with falsely inflated PageRank.'
# b='PageRank is a link analysis algorithm used by the Google Internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of "measuring" its relative importance within the set. The algorithm may be applied to any collection of entities with reciprocal quotations and references. The numerical weight that it assigns to any given element E is also called the PageRank of E and denoted by PR(E). The name "PageRank" is a trademark of Google, and the PageRank process has been patented (U.S. Patent 6,285,999 ). However, the patent is assigned to Stanford University and not to Google. Google has exclusive license rights on the patent from Stanford University. The university received 1.8 million shares in Google in exchange for use of the patent; the shares were sold in 2005 for $336 million. Google describes PageRank: “ PageRank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual pages value. In essence, Google interprets a link from page A to page B as a vote, by page A, for page B. But, Google looks at more than the sheer volume of votes, or links a page receives; it also analyzes the page that casts the vote. Votes cast by pages that are themselves "important" weigh more heavily and help to make other pages "important".” In other words, a PageRank results from a "ballot" among all the other pages on the World Wide Web about how important a page is. A hyperlink to a page counts as a vote of support. The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it ("incoming links"). A page that is linked to by many pages with high PageRank receives a high rank itself. If there are no links to a web page there is no support for that page. Google assigns a numeric weighting from 0-10 for each webpage on the Internet; this PageRank denotes a site’s importance in the eyes of Google. The PageRank is derived from a theoretical probability value on a logarithmic scale like the Richter Scale. The PageRank of a particular page is roughly based upon the quantity of inbound links as well as the PageRank of the pages providing the links. It is known that other factors, e.g. relevance of search words on the page and actual visits to the page reported by the Google toolbar also influence the PageRank. In order to prevent manipulation, spoofing and Spamdexing, Google provides no specific details about how other factors influence PageRank. Numerous academic papers concerning PageRank have been published since Page and Brins original paper. In practice, the PageRank concept has proven to be vulnerable to manipulation, and extensive research has been devoted to identifying falsely inflated PageRank and ways to ignore links from documents with falsely inflated PageRank. Other link-based ranking algorithms for Web pages include the HITS algorithm invented by Jon Kleinberg (used by Teoma and now Ask.com), the IBM CLEVER project, and the TrustRank algorithm.'

# #%%
# # Task C
# c__1='The vector space model (also called, term vector model) is an algebraic model used to represent text documents, as well as any objects in general, as vectors of identifiers. It is used in information retrieval and was first used in the SMART Information Retrieval System. A document is represented as a vector and each dimension corresponds to a separate term. If a term appears in the document then its value in the vector is non-zero. Many different ways of calculating these values, also known as (term) weights, have been developed. One of the best known methods is called tf-idf weighting. The definition of term depends on the application but generally terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary, which is the number of distinct words occurring in the corpus. The vector space model has several disadvantages. Firstly, long documents are represented badly because they have poor similarity values. Secondly, search keywords must accurately match document terms and substrings of words might result in a "false-positive match". Thirdly, documents with similar context but different term vocabulary will not be associated, resulting in a "false-negative match". Finally, the order in which the terms appear in the document is lost in the vector space representation. '
# c__2='The vector space model is an algebraic model used to represent text documents (and any objects, generally) as vectors of identifiers, for instance index terms. Its applications include information filtering, information retrieval, indexing and relevancy rankings. With reference to this model, documents are represented as vectors. Each dimension corresponds to a separate term. The value of a vector is non-zero if a term occurs in the document. Several different ways have been developed of calculating these values (also known as term weights). One of the best known schemes is tf-idf (term frequency-inverse document frequency) weighting. The model can be used to determine the relevancy rankings of documents in a keyword search, using the assumptions of document similarities theory, by comparing the original query vector (where the query is represented as same kind of vector as the documents) and the deviation of angles between each document vector. The classic vector space model was put forward by Salton, Wong and Yang and is known as term frequency-inverse document frequency model. In this classic model the term specific weights in the document vectors are products of local and global parameters. In a simpler Term Count Model the term specific weights are just the counts of term occurrences and therefore do not include the global parameter. '
# c__3='A Vector space model (or term vector model) is an algebraic way of representing text documents (and any objects, in general) as vectors of identifiers, such as index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first application was in the SMART Information Retrieval System. A document can be represented as a vector. Every dimension relates to a different term. If a term appears in the document, the terms value in the vector is non-zero. Many different methods of calculating these values, sometimes known as (term) weights, have been developed. tf-idf weighting is one of the most well known schemes. (see below example). The definition of a term depends on the application. Normally a term is a single word, keyword, or a longer phrase. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). The vector space model has some limitations: 1.	Longer documents are represented poorly because the documents have poor similarity values (namely a small scalar product and a large dimensionality) 2.	Search keywords have to precisely match document terms; word substrings could potentially result in a "false positive match" 3.	Semantic sensitivity; documents with a similar context, but different term vocabulary wont be associated, resulting in a "false negative match". 4.	The order in which terms appear in the document is lost in a vector space representation.'
# c__4='Vector space model, or term vector model as it is also known, is an algebraic model for representing objects (although it is mainly used for text documents) as vectors of identifiers; for example, index terms. It is used in information retrieval and filtering, indexing and relevancy rankings, and was first used in the SMART Information Retrieval System. A document is represented as a vector, with each dimension corresponding to a separate term. If a term occurs in the document, the value will be non-zero in the vector. Many different ways of computing these values (aka (term) weights) have been developed; one of the best known schemes is tf-idf weighting. The way that a term is defined depends on the application. Typically, terms are single words, keywords, or sometimes even longer phrases. If the words are chosen as the terms, the number of dimensions in the vector is the number of distinct words in the corpus. Relevancy ranks for documents, in a keyword search, can be calculated; this uses the assumptions of document similarities theory, by comparing the difference of angles between each document vector and the original query vector, where the query is represented as same format vector as the documents. Generally, it is easier to calculate the cosine of the angle between the vectors instead of the angle itself. A zero value for the cosine indicates that the query and document vector are orthogonal and so had no match; this means the query term did not exist in the document being considered. However, the vector space model has limitations. Long documents are poorly represented due to their poor similarity values (a small scalar product and a large dimensionality); search keywords must match precisely the document terms; word substrings might result in a "false positive match"; similar context documents but different term vocabulary wont be associated, leading to a "false negative match"; and the order that the terms appear in the document is not represented in the vector space model.'
# c='Vector space model (or term vector model) is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System. A document is represented as a vector. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting (see the example below). The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). The vector space model has the following limitations:    1. Long documents are poorly represented because they have poor similarity values (a small scalar product and a large dimensionality)    2. Search keywords must precisely match document terms; word substrings might result in a "false positive match"    3. Semantic sensitivity; documents with similar context but different term vocabulary wont be associated, resulting in a "false negative match". 4. The order in which the terms appear in the document is lost in the vector space representation.'

# #%%
# # Task D
# d__1="Bayes theorem relates the conditional and marginal probabilities of two random events. For example, a person may be seen to have certain medical symptoms; Bayes theorem can then be used to compute the probability that, given that observation, the proposed diagnosis is the right one. Bayes theorem forms a relationship between the probabilities xcof events A and B. Intuitively, Bayes theorem in this form describes the way in which ones recognition of A are updated by having observed 'B'. P(A | B) = P(B | A) P(A) / P(B). P(A|B) is the conditional probability of A given B. It is derived from or depends upon the specified value of B, therefore it is also known as the posterior probability. P(B|A) is the conditional probability of B given A. P(A) is the prior probability A. It doesnt take into account any information about B, so it is prior. P(B) is the prior or marginal probability of B, and acts to normalise the probability. To derive the theorem, we begin with the definition of conditional probability. By combining and re-arranging these two equations for A and B, we get a the lemma called product rule for probabilities. Provided that P(B) is not a zero, dividing both sides by P(B) renders us with Bayes theorem."
# d__2='Bayes’ theorem relates the conditional and marginal probabilities of two random events.  It is mainly used to calculate the probability of one event’s outcome given that a previous event happened.  For example, the probability that a doctors diagnosis is correct given that the doctor had previously observed symptoms in the patient.  Bayes’ theorem can be used for all forms of probability, however it is currently at the centre of a debate concerning the ways in which probabilities should be assigned in applications. The theorem states that the probability of Event A happening given Event B is the probability of B given A multiplied by the probability of A regardless of B all divided by the probability of B regardless of A which acts as a normalising constant.  Bayes’ theorem formed in this way basically details how one’s beliefs about Event A are renewed or updated knowing that Event B happened.  When calculating conditional probabilities such as these, it is often useful to create a table containing the number of occurrences, or relative frequencies, of each outcome for each of the variables independently.'
# d__3='In probability theory, Bayes theorem also called Bayes law after Rev Thomas Bayes compares the conditional and marginal probabilities of two random events. It is often used to calculate posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes theorem can be used to calculate the likelihood that a proposed analysis is accurate, given that observation.  As an official theorem, Bayes theorem is valid in all universal interpretations of probability. However, it plays a fundamental role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole. Whilst Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail. Bayes theorem compares the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability. Each term in Bayes theorem has a conventional name: P(A) is the previous probability  of A. It is "previous" in the sense that it does not take into account any information about B. P(A|B) is the conditional probability of A, given B. It is also called the subsequent probability because it is derived from or depends upon the specified value of B. P(B|A) is the conditional probability of B given A. P(B) is the previous.'
# d__4='Bayes theorem (often called Bayes law) connects the conditional and marginal probabilities of two arbitrary events. One of its uses is calculating posterior probabilities given observations. Bayes theorem plays a key role in the debate around the principles of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Bayes theorem is useful in evaluating the result of drug tests.  If a test can identify a drug user 99% of the time, and can identify a non-user as testing negative 99% of the time, it may seem to be a relatively accurate test.  However, Bayes theorem will reveal the flaw that despite the apparently high accuracy of the test, the probability that an employee who tested positive actually did use drugs is only about 33%.'
# d__5="In probability theory, Bayes theorem (or Bayes law after Rev Thomas Bayes) provides relation between the conditional and marginal probabilities of two random events. It is usually used to calculate posterior probabilities given observations. For example: a patient might be observed to show certain symptoms. Bayes theorem could be used to compute the probability that a certain diagnosis is right, given that observation. Since it is a formal theorem, Bayes theorem holds in all popular interpretations of probability. Bayes theorem relates the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability:  P(a|b) = P(a|b)P(a)/P(b) Terms in Bayes theorem are named by a convention: P(A) is the prior probability or marginal probability of A. It does not take into account any information about B and therefore is considered “prior”. P(A|B) is the conditional probability of A, given B. It it is derived from or depends upon the specified value of B. Usually it is called the posterior probability P(B|A) is the conditional probability of B given A. P(B) (a.k.a. the normalizing constant) is the prior or marginal probability of B. Obviously, Bayes theorem describes the way in which ones assumptions about observing the event a are changed by having observed the event 'b'."
# d='In probability theory, Bayes theorem (often called Bayes law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. (See example 2) As a formal theorem, Bayes theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail. Bayes theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability: P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}. Each term in Bayes theorem has a conventional name: * P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B. * P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B. * P(B|A) is the conditional probability of B given A. * P(B) is the prior or marginal probability of B, and acts as a normalizing constant. Intuitively, Bayes theorem in this form describes the way in which ones beliefs about observing A are updated by having observed B.'

# #%%
# # Task E
# e__1='In computer science; dynamic programming is a way of solving problems consist of overlapping subproblems and optimal substructure. The method is more effiecent than naive methods. The term was first coined in the 1940s by Richard Bellman to describe the process of solving problems where you need to find the best decisions consecutavly. In 1953 he had refined this to the modern meaning. The field was founded as a systems analysis and engineering topic that is recognized by the IEEE. Bellman equation is a central result of dynamic programming which restates an optimization problem in recursive form. dynamic programming has little connection to computer programming at all, and instead comes from the term mathematical programming, a synonym for optimization. Thus, the program is the best plan for action that is produced. For instance, a events schedule at an exhibition is sometimes called a program. Programming means finding a plan of action. '
# e__2='Dynamic programming is a problem-solving method which solves recursive problems. The term is derived from mathematical programming which is commonly referred to as optimisation, hence dynamic programming is an optimal method of solving the problems and takes much less time than naïve methods. Dynamic programming uses the properties of optimal substructure, overlapping subproblems and memoization to create an algorithm to solve such problems. Optimal substructure means that the structure of the problem is made up of sub-problems which can be used to find the solution to the problem overall. A problem with overlapping subproblems means that the same subproblems may be used to solve many different larger problems. Each sub-problem is solved by being divided into sub-subproblems, until a case is reached which is solvable in constant time. Memoization stores solutions which have already been computed in order to reduce unnecessary re-computation. Dynamic programming can be divided into two main approaches: top-down and bottom-up. The top-down approach breaks the problem into subproblems, which are solved and remembered, using a combination of memoization and recursion. The bottom-up approach solves all subproblems that might be need in advance, and then uses these solutions to build up the solutions to the bigger problem. '
# e__3='In computer science and mathematics, dynamic programming is a method of problem solving that utilises the properties of overlapping subproblems and optimal substructure. And thus the method takes much less time than more naive methods. In "dynamic programming", the word "programming" has no real connection to computer programming at all, it actually comes from the term "mathematical programming", a synonym for optimisation. Thus, the "program" is the optimal plan of action that is being produced. For example, a schedule of events at an exhibition is sometimes called a programme. Programming, in this sense, means finding an acceptable plan, an algorithm.'
# e__4='In mathematics and computer science, dynamic progrkmamming is a methodology of the solution of the problems that exhibit the properties of overlapping subproblems and optimal substructure (described below). The methodology takes much less time rather than naive methods. The term was originally used during the 1940s by Richard Bellman to describe the process of solving problems where one needs to find the best decisions one after another. By 1953, he had refined this to the modern meaning. The field was founded as a systems analysis and engineering topic that is recognized by the IEEE. Bellmans contribution is remembered in the name of the Bellman equation, a central result of dynamic programmer, which restates an optimization problem in recursive form. The word "programming" in "dynamic programming" has no particular connection to computer programming in general , and instead of this it comes from the term "mathematical programming", a synonym for optimization. Therefore, the "program" is the optimal plan for action that is produced. For example, a finalized schedule of events at an exhibition is sometimes called a program. Optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem. For instance, the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices. After this, it is using this to pick the best overall path. In a word, we can solve a problem with optimal substructure using a three-step process.'
# e='In mathematics and computer science, dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure (described below). The method takes much less time than naive methods. The term was originally used in the 1940s by Richard Bellman to describe the process of solving problems where one needs to find the best decisions one after another. By 1953, he had refined this to the modern meaning. The field was founded as a systems analysis and engineering topic that is recognized by the IEEE. Bellmans contribution is remembered in the name of the Bellman equation, a central result of dynamic programming which restates an optimization problem in recursive form. The word "programming" in "dynamic programming" has no particular connection to computer programming at all, and instead comes from the term "mathematical programming", a synonym for optimization. Thus, the "program" is the optimal plan for action that is produced. For instance, a finalized schedule of events at an exhibition is sometimes called a program. Programming, in this sense, means finding an acceptable plan of action, an algorithm. Optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem. For example, the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices, and then using this to pick the best overall path, as shown in Figure 1. In general, we can solve a problem with optimal substructure using a three-step process: 1. Break the problem into smaller subproblems. 2. Solve these problems optimally using this three-step process recursively. 3. Use these optimal solutions to construct an optimal solution for the original problem. The subproblems are, themselves, solved by dividing them into sub-subproblems, and so on, until we reach some simple case that is solvable in constant time. Figure 2. The subproblem graph for the Fibonacci sequence. That it is not a tree but a DAG indicates overlapping subproblems. To say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems. For example, in the Fibonacci sequence, F3 = F1 + F2 and F4 = F2 + F3 — computing each number involves computing F2. Because both F3 and F4 are needed to compute F5, a naive approach to computing F5 may end up computing F2 twice or more. This applies whenever overlapping subproblems are present: a naive approach may waste time recomputing optimal solutions to subproblems it has already solved. In order to avoid this, we instead save the solutions to problems we have already solved. Then, if we need to solve the same problem later, we can retrieve and reuse our already-computed solution. This approach is called memoization (not memorization, although this term also fits). If we are sure we wont need a particular solution anymore, we can throw it away to save space. In some cases, we can even compute the solutions to subproblems we know that well need in advance.'











